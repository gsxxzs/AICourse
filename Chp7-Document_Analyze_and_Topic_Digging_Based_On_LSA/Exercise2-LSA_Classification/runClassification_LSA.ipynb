{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "  4743 training examples (590 positive)\n",
      "  4858 test examples (620 positive)\n",
      "  Actual number of tfidf features: 10000\n",
      "\n",
      "Performing dimensionality reduction using LSA\n",
      "  done in 1.310sec\n",
      "  Explained variance of the SVD step: 27%\n",
      "\n",
      "Classifying tfidf vectors...\n",
      "  (4471 / 4858) correct - 92.03%\n",
      "  done in 1.026sec\n",
      "\n",
      "Classifying LSA vectors...\n",
      "  (4545 / 4858) correct - 93.56%\n",
      "    done in 0.520sec\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Run k-NN classification on the Reuters text dataset using LSA.\n",
    "\n",
    "This script leverages modules in scikit-learn for performing tf-idf and SVD.\n",
    "\n",
    "Classification is performed using k-NN with k=5 (majority wins).\n",
    "\n",
    "The script measures the accuracy of plain tf-idf as a baseline, then LSA to\n",
    "show the improvement.\n",
    "\n",
    "@author: Chris McCormick\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#  Load the raw text dataset.\n",
    "###############################################################################\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# The raw text dataset is stored as tuple in the form:\n",
    "# (X_train_raw, y_train_raw, X_test_raw, y_test)\n",
    "# The 'filtered' dataset excludes any articles that we failed to retrieve\n",
    "# fingerprints for.\n",
    "raw_text_dataset = pickle.load( open( \"./data/raw_text_dataset.pickle\", \"rb\" ) )\n",
    "X_train_raw = raw_text_dataset[0]\n",
    "y_train_labels = raw_text_dataset[1] \n",
    "X_test_raw = raw_text_dataset[2]\n",
    "y_test_labels = raw_text_dataset[3]\n",
    "\n",
    "# The Reuters dataset consists of ~100 categories. However, we are going to\n",
    "# simplify this to a binary classification problem. The 'positive class' will\n",
    "# be the articles related to \"acquisitions\" (or \"acq\" in the dataset). All\n",
    "# other articles will be negative.\n",
    "y_train = [\"acq\" in y for y in y_train_labels]\n",
    "y_test = [\"acq\" in y for y in y_test_labels]\n",
    "\n",
    "print(\"  %d training examples (%d positive)\" % (len(y_train), sum(y_train)))\n",
    "print(\"  %d test examples (%d positive)\" % (len(y_test), sum(y_test)))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#  Use LSA to vectorize the articles.\n",
    "###############################################################################\n",
    "\n",
    "# Tfidf vectorizer:\n",
    "#   - Strips out “stop words”\n",
    "#   - Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "#   - Filters out terms that occur in only one document (min_df=2).\n",
    "#   - Selects the 10,000 most frequently occuring words in the corpus.\n",
    "#   - Normalizes the vector (L2 norm of 1.0) to normalize the effect of \n",
    "#     document length on the tf-idf values. \n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)\n",
    "\n",
    "# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n",
    "# (\"transform\").\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "\n",
    "print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[1])\n",
    "\n",
    "print(\"\\nPerforming dimensionality reduction using LSA\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Project the tfidf vectors onto the first N principal components.\n",
    "# Though this is significantly fewer features than the original tfidf vector,\n",
    "# they are stronger features, and the accuracy is higher.\n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "\n",
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#  Run classification of the test articles\n",
    "###############################################################################\n",
    "\n",
    "print(\"\\nClassifying tfidf vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_tfidf = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"  done in %.3fsec\" % elapsed)\n",
    "\n",
    "\n",
    "print(\"\\nClassifying LSA vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(X_test_lsa)\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
