{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1b2432374592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m#doc = '十四是十四四十是四十，，十四不是四十，，，，四十不是十四'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordSegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_word_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_aggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_entropy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%s:%f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_with_freq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutPut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1b2432374592>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, doc, max_word_len, min_freq, min_entropy, min_aggregation)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_aggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_aggregation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Result infomations, i.e., average data of all words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1b2432374592>\u001b[0m in \u001b[0;36mgenWords\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_cands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mword_cands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mword_cands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;31m# compute probability and entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1b2432374592>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, left, right)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "\"\"\"\n",
    "Chinese word segmentation algorithm without corpus\n",
    "Author: 段凯强\n",
    "Reference: http://www.matrix67.com/blog/archives/5044\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(r'/home/tianyi/chp7/Py3/ChineseWordSegmentation-master/wordseg')\n",
    "\n",
    "from probability import entropyOfList\n",
    "from sequence import genSubparts, genSubstr\n",
    "\n",
    "import json\n",
    "import chardet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indexOfSortedSuffix(doc, max_word_len):\n",
    "    \"\"\"\n",
    "    Treat a suffix as an index where the suffix begins.\n",
    "    Then sort these indexes by the suffixes.\n",
    "    \"\"\"\n",
    "    indexes = []\n",
    "    length = len(doc)\n",
    "    for i in range(0, length):\n",
    "        for j in range(i + 1, min(i + 1 + max_word_len, length + 1)):\n",
    "            indexes.append((i, j))\n",
    "    return sorted(indexes, key=lambda i_j: doc[i_j[0]:i_j[1]])\n",
    "\n",
    "\n",
    "class WordInfo(object):\n",
    "    \"\"\"\n",
    "    Store information of each word, including its freqency, left neighbors and right neighbors\n",
    "    \"\"\"\n",
    "    def __init__(self, text):\n",
    "        super(WordInfo, self).__init__()\n",
    "        self.text = text\n",
    "        self.freq = 0.0\n",
    "        self.left = []\n",
    "        self.right = []\n",
    "        self.aggregation = 0\n",
    "\n",
    "    def update(self, left, right):\n",
    "        \"\"\"\n",
    "        Increase frequency of this word, then append left/right neighbors\n",
    "        @param left a single character on the left side of this word\n",
    "        @param right as left is, but on the right side\n",
    "        \"\"\"\n",
    "        self.freq += 1\n",
    "        if left: self.left.append(left)\n",
    "        if right: self.right.append(right)\n",
    "\n",
    "    def compute(self, length):\n",
    "        \"\"\"\n",
    "        Compute frequency and entropy of this word\n",
    "        @param length length of the document for training to get words\n",
    "        \"\"\"\n",
    "        self.freq /= length\n",
    "        self.left = entropyOfList(self.left)\n",
    "        self.right = entropyOfList(self.right)\n",
    "\n",
    "    def computeAggregation(self, words_dict):\n",
    "        \"\"\"\n",
    "        Compute aggregation of this word\n",
    "        @param words_dict frequency dict of all candidate words\n",
    "        \"\"\"\n",
    "        parts = genSubparts(self.text)\n",
    "        if len(parts) > 0:\n",
    "            self.aggregation = min([self.freq/words_dict[p1_p2[0]].freq/words_dict[p1_p2[1]].freq for p1_p2 in parts])\n",
    "\n",
    "\n",
    "\n",
    "class WordSegment(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Main class for Chinese word segmentation\n",
    "    1. Generate words from a long enough document\n",
    "    2. Do the segmentation work with the document\n",
    "    \"\"\"\n",
    "\n",
    "    # if a word is combination of other shorter words, then treat it as a long word\n",
    "    L = 0\n",
    "    # if a word is combination of other shorter words, then treat it as the set of shortest words\n",
    "    S = 1\n",
    "    # if a word contains other shorter words, then return all possible results\n",
    "    ALL = 2\n",
    "\n",
    "    def __init__(self, doc, max_word_len=5, min_freq=0.00005, min_entropy=2.0, min_aggregation=50):\n",
    "        super(WordSegment, self).__init__()\n",
    "        self.max_word_len = max_word_len\n",
    "        self.min_freq = min_freq\n",
    "        self.min_entropy = min_entropy\n",
    "        self.min_aggregation = min_aggregation\n",
    "        self.word_infos = self.genWords(doc)\n",
    "        # Result infomations, i.e., average data of all words\n",
    "        word_count = float(len(self.word_infos))\n",
    "        self.avg_len = sum([len(w.text) for w in self.word_infos])/word_count\n",
    "        self.avg_freq = sum([w.freq for w in self.word_infos])/word_count\n",
    "        self.avg_left_entropy = sum([w.left for w in self.word_infos])/word_count\n",
    "        self.avg_right_entropy = sum([w.right for w in self.word_infos])/word_count\n",
    "        self.avg_aggregation = sum([w.aggregation for w in self.word_infos])/word_count\n",
    "        # Filter out the results satisfy all the requirements\n",
    "        filter_func = lambda v: len(v.text) > 1 and v.aggregation > self.min_aggregation and\\\n",
    "                    v.freq > self.min_freq and v.left > self.min_entropy and v.right > self.min_entropy\n",
    "        self.word_with_freq = [(w.text, w.freq) for w in list(filter(filter_func, self.word_infos))]\n",
    "        self.words = [w[0] for w in self.word_with_freq]\n",
    "\n",
    "    def genWords(self, doc):\n",
    "        \"\"\"\n",
    "        Generate all candidate words with their frequency/entropy/aggregation informations\n",
    "        @param doc the document used for words generation\n",
    "        \"\"\"\n",
    "        pattern = re.compile('[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!@#$%^&*\\\\-_=+a-zA-Z，。《》、？：；“”‘’｛｝【】（）…￥！—┄－]+')\n",
    "        doc = re.sub(pattern, ' ', doc)\n",
    "        suffix_indexes = indexOfSortedSuffix(doc, self.max_word_len)\n",
    "        word_cands = {}\n",
    "        # compute frequency and neighbors\n",
    "        for suf in suffix_indexes:\n",
    "            word = doc[suf[0]:suf[1]]\n",
    "            if word not in word_cands:\n",
    "                word_cands[word] = WordInfo(word)\n",
    "            word_cands[word].update(doc[suf[0] - 1:suf[0]], doc[suf[1]:suf[1] + 1])\n",
    "        # compute probability and entropy\n",
    "        length = len(doc)\n",
    "        for k in word_cands:\n",
    "            word_cands[k].compute(length)\n",
    "        # compute aggregation of words whose length > 1\n",
    "        values = sorted(list(word_cands.values()), key=lambda x: len(x.text))\n",
    "        for v in values:\n",
    "            if len(v.text) == 1: continue\n",
    "            v.computeAggregation(word_cands)\n",
    "        return sorted(values, key=lambda v: v.freq, reverse=True)\n",
    "\n",
    "    def segSentence(self, sentence, method=ALL):\n",
    "        \"\"\"\n",
    "        Segment a sentence with the words generated from a document\n",
    "        @param sentence the sentence to be handled\n",
    "        @param method segmentation method\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        res = []\n",
    "        while i < len(sentence):\n",
    "            if method == self.L or method == self.S:\n",
    "                j_range = list(range(self.max_word_len, 0, -1)) if method == self.L else list(range(2, self.max_word_len + 1)) + [1]\n",
    "                for j in j_range:\n",
    "                    if j == 1 or sentence[i:i + j] in self.words:\n",
    "                        res.append(sentence[i:i + j])\n",
    "                        i += j\n",
    "                        break\n",
    "            else:\n",
    "                to_inc = 1\n",
    "                for j in range(2, self.max_word_len + 1):\n",
    "                    if i + j <= len(sentence) and sentence[i:i + j] in self.words:\n",
    "                        res.append(sentence[i:i + j])\n",
    "                        if to_inc == 1: to_inc = j\n",
    "                if to_inc == 1: res.append(sentence[i])\n",
    "                i += to_inc\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "outPut=open('outPut.txt','w')    \n",
    "    \n",
    "    \n",
    "with open('../../../data/Chp7/news2016zh_valid.json', encoding='utf-8') as d:\n",
    "    doc = d.readlines()\n",
    "    \n",
    "#print(chardet.detect(doc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for line in doc:\n",
    "    #doc = '十四是十四四十是四十，，十四不是四十，，，，四十不是十四'\n",
    "    ws = WordSegment(line, max_word_len=2, min_aggregation=50, min_entropy=1.5)\n",
    "    \n",
    "    print((' '.join(['%s:%f'%w for w in ws.word_with_freq])),file=outPut)\n",
    "    print((' '.join(ws.words)),file=outPut)\n",
    "    print((' '.join(ws.segSentence(line))),file=outPut)\n",
    "    print(('average len: ', ws.avg_len),file=outPut)\n",
    "    print(('average frequency: ', ws.avg_freq),file=outPut)\n",
    "    print(('average left entropy: ', ws.avg_left_entropy),file=outPut)\n",
    "    print(('average right entropy: ', ws.avg_right_entropy),file=outPut)\n",
    "    print(('average aggregation: ', ws.avg_aggregation),file=outPut)\n",
    "\n",
    "outPut.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
